import pandas as pd
import os
import re
import hashlib
from datetime import date, datetime
from typing import List, Dict, Optional, Tuple
from pathlib import Path
from dateutil import parser as date_parser
import asyncio
from concurrent.futures import ThreadPoolExecutor
import threading
from app.services.mock_data import generate_mock_products
from app.services.sqlite_cache import SQLiteCache


class ExcelLoader:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Excel —Ñ–∞–π–ª–æ–≤"""
    
    def __init__(self, data_dir: str = "data"):
        self.data_dir = Path(data_dir)
        self._cache: Optional[pd.DataFrame] = None
        self._file_metadata: Dict[str, Dict] = {}
        self._loading = False
        self._load_lock = threading.Lock()
        self._executor = ThreadPoolExecutor(max_workers=4)
        self._using_mock_data = False
        self._data_ready = False
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º SQLite –∫—ç—à
        cache_dir = os.getenv("CACHE_DIR", "cache")
        if not os.path.isabs(cache_dir):
            base_dir = Path(__file__).parent.parent.parent
            cache_dir = str(base_dir / cache_dir)
        self._sqlite_cache = SQLiteCache(cache_dir)
    
    def _parse_filename_dates(self, filename: str) -> Tuple[Optional[date], Optional[date]]:
        """–ü–∞—Ä—Å–∏—Ç –¥–∞—Ç—ã –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞"""
        period_start = None
        period_end = None
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –Ω–∞–∑–≤–∞–Ω–∏–π —Ñ–∞–π–ª–æ–≤
        patterns = [
            # –§–æ—Ä–º–∞—Ç: chto-dobavlyaut-v-izbrannoe_-06_03_2021-04_04_2021.xlsx
            (r'(\d{2})_(\d{2})_(\d{4})-(\d{2})_(\d{2})_(\d{4})', 
             lambda m: (date(int(m.group(3)), int(m.group(2)), int(m.group(1))),
                       date(int(m.group(6)), int(m.group(5)), int(m.group(4))))),
            # –§–æ—Ä–º–∞—Ç: chto-dobavlyali-v-izbrannoe-v-dekabre-2020.xlsx
            (r'dekabre-(\d{4})', lambda m: (date(int(m.group(1)), 12, 1), date(int(m.group(1)), 12, 31))),
            (r'noyabre-(\d{4})', lambda m: (date(int(m.group(1)), 11, 1), date(int(m.group(1)), 11, 30))),
            (r'yanvare-(\d{4})', lambda m: (date(int(m.group(1)), 1, 1), date(int(m.group(1)), 1, 31))),
            # –§–æ—Ä–º–∞—Ç: 2021-07-12_opendata_datasetfavorites_2021-06-12_2021-07-11.xlsx
            (r'(\d{4}-\d{2}-\d{2})_opendata.*?(\d{4}-\d{2}-\d{2})_(\d{4}-\d{2}-\d{2})',
             lambda m: (date_parser.parse(m.group(2)).date(), date_parser.parse(m.group(3)).date())),
        ]
        
        for pattern, func in patterns:
            match = re.search(pattern, filename)
            if match:
                try:
                    period_start, period_end = func(match)
                    return period_start, period_end
                except:
                    continue
        
        return None, None
    
    def _normalize_columns(self, df: pd.DataFrame, filename: str) -> pd.DataFrame:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö"""
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫
        column_mapping = {
            '–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞': 'name',
            '–ë—Ä–µ–Ω–¥': 'brand',
            '–°—Å—ã–ª–∫–∞ –Ω–∞ —Ç–æ–≤–∞—Ä': 'link',
            '–ö–∞—Ç–µ–≥–æ—Ä–∏—è 1 —É—Ä–æ–≤–Ω—è': 'category_level_1',
            '–ö–∞—Ç–µ–≥–æ—Ä–∏—è 2 —É—Ä–æ–≤–Ω—è': 'category_level_2',
            '–ö–∞—Ç–µ–≥–æ—Ä–∏—è 3 —É—Ä–æ–≤–Ω—è': 'category_level_3',
            '–ö–∞—Ç–µ–≥–æ—Ä–∏—è 4 —É—Ä–æ–≤–Ω—è': 'category_level_4',
        }
        
        # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏
        df_normalized = df.rename(columns=column_mapping)
        
        # –ù–∞—Ö–æ–¥–∏–º –∫–æ–ª–æ–Ω–∫—É —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–æ–±–∞–≤–ª–µ–Ω–∏–π –≤ –∏–∑–±—Ä–∞–Ω–Ω–æ–µ
        favorites_col = None
        for col in df.columns:
            if '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–±–∞–≤–ª–µ–Ω–∏–π' in col or '–¥–æ–±–∞–≤–ª–µ–Ω–∏–π –≤ –∏–∑–±—Ä–∞–Ω–Ω–æ–µ' in col:
                favorites_col = col
                break
        
        if favorites_col:
            df_normalized['favorites_count'] = df[favorites_col]
        else:
            df_normalized['favorites_count'] = 0
        
        # –ù–∞—Ö–æ–¥–∏–º –∫–æ–ª–æ–Ω–∫—É —Å –ø–æ—Å–ª–µ–¥–Ω–∏–º –ø–æ—è–≤–ª–µ–Ω–∏–µ–º –≤ –Ω–∞–ª–∏—á–∏–∏
        stock_col = None
        for col in df.columns:
            if '–ü–æ—Å–ª–µ–¥–Ω–µ–µ –ø–æ—è–≤–ª–µ–Ω–∏–µ' in col or '–ø–æ—è–≤–ª–µ–Ω–∏–µ –≤ –Ω–∞–ª–∏—á–∏–∏' in col:
                stock_col = col
                break
        
        if stock_col:
            df_normalized['last_in_stock'] = pd.to_datetime(df[stock_col], errors='coerce').dt.date
        else:
            df_normalized['last_in_stock'] = None
        
        # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—ã –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞
        period_start, period_end = self._parse_filename_dates(filename)
        df_normalized['period_start'] = period_start
        df_normalized['period_end'] = period_end
        
        # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è —Ç–æ–≤–∞—Ä–∞
        def create_product_id(row):
            """–°–æ–∑–¥–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–∑–≤–∞–Ω–∏—è, –±—Ä–µ–Ω–¥–∞ –∏ —Å—Å—ã–ª–∫–∏"""
            key = f"{row.get('name', '')}|{row.get('brand', '')}|{row.get('link', '')}"
            return hashlib.md5(key.encode()).hexdigest()[:16]
        
        df_normalized['id'] = df_normalized.apply(create_product_id, axis=1)
        
        # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        required_cols = ['id', 'name', 'brand', 'link', 'category_level_1', 
                        'category_level_2', 'category_level_3', 'category_level_4',
                        'favorites_count', 'last_in_stock', 'period_start', 'period_end']
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏
        for col in required_cols:
            if col not in df_normalized.columns:
                df_normalized[col] = None
        
        return df_normalized[required_cols]
    
    def _calculate_days_out_of_stock(self, df: pd.DataFrame) -> pd.DataFrame:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –≤ –Ω–∞–ª–∏—á–∏–∏"""
        today = date.today()
        
        def calc_days(row):
            if pd.isna(row['last_in_stock']) or row['last_in_stock'] is None:
                return None
            delta = today - row['last_in_stock']
            return delta.days if delta.days >= 0 else 0
        
        df['days_out_of_stock'] = df.apply(calc_days, axis=1)
        return df
    
    def _load_single_file(self, file_path: Path) -> Optional[Tuple[pd.DataFrame, Dict]]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–¥–∏–Ω Excel —Ñ–∞–π–ª"""
        try:
            df = pd.read_excel(file_path, engine='openpyxl')
            df_normalized = self._normalize_columns(df, file_path.name)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª–∞
            period_start, period_end = self._parse_filename_dates(file_path.name)
            metadata = {
                'period_start': period_start,
                'period_end': period_end,
                'rows_count': len(df)
            }
            
            return df_normalized, metadata
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞ {file_path.name}: {e}")
            return None
    
    def load_all_data(self, force_reload: bool = False) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ Excel —Ñ–∞–π–ª–æ–≤ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–æ–π"""
        with self._load_lock:
            if self._cache is not None and not force_reload:
                return self._cache
            
            if self._loading:
                # –ï—Å–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ —É–∂–µ –∏–¥–µ—Ç, –∂–¥–µ–º –µ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
                while self._loading:
                    import time
                    time.sleep(0.1)
                if self._cache is not None:
                    return self._cache
            
            self._loading = True
        
        # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ SQLite –∫—ç—à–∞
        if not force_reload:
            cached_df = self._sqlite_cache.get_cached_data(self.data_dir)
            if cached_df is not None:
                with self._load_lock:
                    self._cache = cached_df
                    self._file_metadata = self._sqlite_cache.get_file_metadata()
                    self._loading = False
                    self._using_mock_data = False
                return cached_df
        
        try:
            if not self.data_dir.exists():
                raise FileNotFoundError(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {self.data_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            
            excel_files = list(self.data_dir.glob("*.xlsx"))
            
            if not excel_files:
                raise FileNotFoundError(f"Excel —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {self.data_dir}")
            
            print(f"–ù–∞—á–∏–Ω–∞—é –∑–∞–≥—Ä—É–∑–∫—É {len(excel_files)} —Ñ–∞–π–ª–æ–≤...")
            
            # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤
            all_dataframes = []
            file_metadata = {}
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º ThreadPoolExecutor –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
            futures = []
            for file_path in excel_files:
                future = self._executor.submit(self._load_single_file, file_path)
                futures.append((future, file_path.name))
            
            # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for future, filename in futures:
                result = future.result()
                if result is not None:
                    df_normalized, metadata = result
                    all_dataframes.append(df_normalized)
                    file_metadata[filename] = metadata
                    print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª: {filename} ({metadata['rows_count']} —Å—Ç—Ä–æ–∫)")
            
            if not all_dataframes:
                raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–æ–≤")
            
            print(f"–û–±—ä–µ–¥–∏–Ω—è—é {len(all_dataframes)} –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–æ–≤...")
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
            combined_df = pd.concat(all_dataframes, ignore_index=True)
            
            print("–í—ã—á–∏—Å–ª—è—é –¥–Ω–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –≤ –Ω–∞–ª–∏—á–∏–∏...")
            
            # –í—ã—á–∏—Å–ª—è–µ–º –¥–Ω–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –≤ –Ω–∞–ª–∏—á–∏–∏
            combined_df = self._calculate_days_out_of_stock(combined_df)
            
            # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            with self._load_lock:
                self._cache = combined_df
                self._file_metadata = file_metadata
                self._loading = False
                self._using_mock_data = False
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ SQLite –∫—ç—à
            self._sqlite_cache.save_data(self.data_dir, combined_df)
            self._sqlite_cache.save_file_metadata(file_metadata)
            
            print(f"‚úì –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {len(combined_df)} —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ {len(file_metadata)} —Ñ–∞–π–ª–æ–≤")
            
            return combined_df
            
        except Exception as e:
            with self._load_lock:
                self._loading = False
            raise e
    
    def get_file_metadata(self) -> Dict[str, Dict]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        if self._cache is None:
            self.load_all_data()
        return self._file_metadata
    
    def clear_cache(self):
        """–û—á–∏—â–∞–µ—Ç –∫—ç—à"""
        with self._load_lock:
            self._cache = None
            self._file_metadata = {}
    
    def load_quick_start_file(self) -> pd.DataFrame:
        """–ë—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º SQLite –∫—ç—à
        print("‚ö° –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç: –ø—Ä–æ–≤–µ—Ä—è—é –∫—ç—à...")
        cached_df = self._sqlite_cache.get_cached_data(self.data_dir)
        
        if cached_df is not None and len(cached_df) > 0:
            print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫—ç—à–∞: {len(cached_df)} —Ç–æ–≤–∞—Ä–æ–≤")
            with self._load_lock:
                self._cache = cached_df
                self._file_metadata = self._sqlite_cache.get_file_metadata()
                self._loading = False
                self._using_mock_data = False
                self._data_ready = True
            return cached_df
        
        # –ï—Å–ª–∏ –∫—ç—à–∞ –Ω–µ—Ç, –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–∫ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–≥–Ω–æ–≤–µ–Ω–Ω–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞
        print("‚ö° –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç: –∑–∞–≥—Ä—É–∂–∞—é –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ...")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –º–æ–∫ –¥–∞–Ω–Ω—ã–µ (1000 —Ç–æ–≤–∞—Ä–æ–≤, 70% —Å days_out_of_stock >= 15)
        mock_df = generate_mock_products(1000)
        
        # –í—ã—á–∏—Å–ª—è–µ–º days_out_of_stock –¥–ª—è –º–æ–∫ –¥–∞–Ω–Ω—ã—Ö
        mock_df = self._calculate_days_out_of_stock(mock_df)
        
        with self._load_lock:
            self._cache = mock_df
            self._file_metadata = {"mock_data": {"rows_count": len(mock_df)}}
            self._loading = False
            self._using_mock_data = True
            self._data_ready = True
        
        high_priority_count = len(mock_df[mock_df['days_out_of_stock'] >= 15])
        print(f"‚úÖ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {len(mock_df)} —Ç–æ–≤–∞—Ä–æ–≤")
        print(f"   üìä –¢–æ–≤–∞—Ä–æ–≤ —Å days_out_of_stock >= 15: {high_priority_count}")
        print("‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. –†–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –≤ —Ñ–æ–Ω–µ.")
        return mock_df
    
    def load_remaining_files_async(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ —Å –∑–∞–º–µ–Ω–æ–π –º–æ–∫ –¥–∞–Ω–Ω—ã—Ö"""
        def load_in_background():
            try:
                import time
                time.sleep(2)  # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞, —á—Ç–æ–±—ã –º–æ–∫ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å—Å—è
                
                print("üîÑ –ù–∞—á–∏–Ω–∞—é –∑–∞–≥—Ä—É–∑–∫—É —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ Excel —Ñ–∞–π–ª–æ–≤...")
                print("üìä –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –±—É–¥—É—Ç –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –∑–∞–º–µ–Ω–µ–Ω—ã –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ")
                
                if not self.data_dir.exists():
                    print("‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
                    return
                
                excel_files = list(self.data_dir.glob("*.xlsx"))
                
                if not excel_files:
                    print("‚ùå Excel —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
                    return
                
                # –ù–∞—á–∏–Ω–∞–µ–º —Å –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ç–∞—Ä—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞
                quick_start_file = "chto-dobavlyaut-v-izbrannoe_-06_03_2021-04_04_2021.xlsx"
                quick_start_path = self.data_dir / quick_start_file
                
                # –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∂–∞–µ–º –±—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç–æ–≤—ã–π —Ñ–∞–π–ª
                if quick_start_path.exists():
                    try:
                        print(f"üì• –ó–∞–≥—Ä—É–∂–∞—é —Å—Ç–∞—Ä—Ç–æ–≤—ã–π —Ñ–∞–π–ª: {quick_start_file}")
                        result = self._load_single_file(quick_start_path)
                        if result is not None:
                            df_normalized, metadata = result
                            df_normalized = self._calculate_days_out_of_stock(df_normalized)
                            
                            with self._load_lock:
                                # –ó–∞–º–µ–Ω—è–µ–º –º–æ–∫ –¥–∞–Ω–Ω—ã–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ
                                self._cache = df_normalized
                                self._file_metadata = {quick_start_file: metadata}
                                self._using_mock_data = False
                            
                            print(f"‚úÖ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {len(df_normalized)} —Ç–æ–≤–∞—Ä–æ–≤")
                            print("üîÑ –ü—Ä–æ–¥–æ–ª–∂–∞—é –∑–∞–≥—Ä—É–∑–∫—É –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤...")
                    except Exception as e:
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç–∞—Ä—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞: {e}")
                
                # –ó–∞—Ç–µ–º –∑–∞–≥—Ä—É–∂–∞–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã –ø–æ –æ–¥–Ω–æ–º—É
                remaining_files = [f for f in excel_files if f.name != quick_start_file]
                
                if remaining_files:
                    print(f"üì¶ –ó–∞–≥—Ä—É–∂–∞—é {len(remaining_files)} –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤...")
                    loaded_count = 0
                    
                    for file_path in remaining_files:
                        try:
                            result = self._load_single_file(file_path)
                            if result is not None:
                                df_normalized, metadata = result
                                loaded_count += 1
                                print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª: {file_path.name} ({metadata['rows_count']} —Å—Ç—Ä–æ–∫)")
                                
                                # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫—ç—à–µ–º
                                with self._load_lock:
                                    if self._cache is not None:
                                        self._cache = pd.concat([self._cache, df_normalized], ignore_index=True)
                                        self._cache = self._calculate_days_out_of_stock(self._cache)
                                    else:
                                        self._cache = df_normalized
                                        self._cache = self._calculate_days_out_of_stock(self._cache)
                                    
                                    if file_path.name not in self._file_metadata:
                                        self._file_metadata[file_path.name] = metadata
                                    
                                    self._using_mock_data = False
                                
                                print(f"  üìä –í—Å–µ–≥–æ —Ç–æ–≤–∞—Ä–æ–≤ –≤ –∫—ç—à–µ: {len(self._cache)}")
                        except Exception as e:
                            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞ {file_path.name}: {e}")
                            continue
                    
                    with self._load_lock:
                        print(f"‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(self._cache)} —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ {len(self._file_metadata)} —Ñ–∞–π–ª–æ–≤")
                        print("‚úÖ –í—Å–µ –¥–∞–Ω–Ω—ã–µ –∑–∞–º–µ–Ω–µ–Ω—ã –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ")
                else:
                    print("‚úÖ –í—Å–µ —Ñ–∞–π–ª—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–æ–Ω–æ–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–µ: {e}")
                print("‚ö†Ô∏è –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
        
        thread = threading.Thread(target=load_in_background, daemon=True)
        thread.start()
        return thread
    
    def preload_data_async(self):
        """–ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö: —Å–Ω–∞—á–∞–ª–∞ –∏–∑ –∫—ç—à–∞/–º–æ–∫, –∑–∞—Ç–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ"""
        # –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫—ç—à–∞ –∏–ª–∏ –º–æ–∫ –¥–∞–Ω–Ω—ã–µ –º–≥–Ω–æ–≤–µ–Ω–Ω–æ
        df = self.load_quick_start_file()
        
        # –ï—Å–ª–∏ –∑–∞–≥—Ä—É–∑–∏–ª–∏ –∏–∑ –∫—ç—à–∞ –∏ —ç—Ç–æ –Ω–µ –º–æ–∫ –¥–∞–Ω–Ω—ã–µ, –Ω–µ –Ω—É–∂–Ω–æ –∑–∞–≥—Ä—É–∂–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ
        if not self._using_mock_data:
            print("‚úÖ –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫—ç—à–∞, –ø—Ä–æ–ø—É—Å–∫–∞—é –∑–∞–≥—Ä—É–∑–∫—É —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
            return None
        
        # –ï—Å–ª–∏ —ç—Ç–æ –º–æ–∫ –¥–∞–Ω–Ω—ã–µ, –∑–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –≤ —Ñ–æ–Ω–µ
        return self.load_remaining_files_async()


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∑–∞–≥—Ä—É–∑—á–∏–∫–∞
_loader_instance: Optional[ExcelLoader] = None


def get_loader(data_dir: Optional[str] = None) -> ExcelLoader:
    """–ü–æ–ª—É—á–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ (singleton)"""
    global _loader_instance
    if _loader_instance is None:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Ç—å –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π
        if data_dir is None:
            import os
            from pathlib import Path
            data_dir = os.getenv("DATA_DIR", "data")
            # –ï—Å–ª–∏ –ø—É—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π, –¥–µ–ª–∞–µ–º –µ–≥–æ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞
            if not os.path.isabs(data_dir):
                base_dir = Path(__file__).parent.parent.parent
                data_dir = str(base_dir / data_dir)
        _loader_instance = ExcelLoader(data_dir)
    return _loader_instance
